{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ee12e2c",
   "metadata": {},
   "source": [
    "# Bangla Long-Form ASR Baseline\n",
    "\n",
    "Minimal baseline using **wav2vec2-large-xlsr-53** with CTC decoding.\n",
    "\n",
    "- **Input**: Long Bangla .wav files\n",
    "- **Output**: Bangla text transcription\n",
    "- **Method**: 25-second chunking, greedy CTC decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20967481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# !pip install -q transformers librosa soundfile torchaudio\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70ecf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 20:24:45.501633: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770063885.873101     102 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770063886.009890     102 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770063886.876627     102 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770063886.876668     102 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770063886.876670     102 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770063886.876673     102 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Denoising enabled: True\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import librosa\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from difflib import SequenceMatcher\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# Configuration\n",
    "BASE_INPUT_DIR = \"/kaggle/input/dl-sprint-4-0-bengali-long-form-speech-recognition/transcription/transcription\"\n",
    "BASE_OUTPUT_DIR = \"/kaggle/working/\"\n",
    "TEST_AUDIO_DIR = os.path.join(BASE_INPUT_DIR, \"test\")\n",
    "SUBMISSION_PATH = os.path.join(BASE_OUTPUT_DIR, \"submission.csv\")\n",
    "\n",
    "MODEL_NAME = \"arijitx/wav2vec2-xls-r-300m-bengali\"  # Bengali-specific model\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "CHUNK_LENGTH_SEC = 15\n",
    "OVERLAP_SEC = 3  # Overlap duration in seconds (recommended: 2-5 seconds)\n",
    "\n",
    "# Spectral Gating Configuration\n",
    "ENABLE_DENOISING = True  # Toggle denoising on/off\n",
    "NOISE_GATE_THRESHOLD_K = 2.0  # Conservative: 1.5-2.5 (higher = less aggressive)\n",
    "STFT_WIN_LENGTH_MS = 25  # ~25ms window\n",
    "STFT_HOP_LENGTH_MS = 10  # ~10ms hop\n",
    "SOFT_MASK_MIN = 0.1  # Minimum mask value (no hard zeroing)\n",
    "\n",
    "# Post-processing Configuration\n",
    "ENABLE_LM_DECODING = False  # N-gram LM decoding (requires KenLM installation)\n",
    "ENABLE_UNICODE_NORMALIZATION = True  # Unicode normalization\n",
    "ENABLE_SENTENCE_END_CHAR = False  # Append period at end if missing\n",
    "SENTENCE_END_CHAR = \"।\"  # Bengali sentence end character (dari)\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Denoising enabled: {ENABLE_DENOISING}\")\n",
    "print(f\"Chunk length: {CHUNK_LENGTH_SEC}s, Overlap: {OVERLAP_SEC}s\")\n",
    "print(f\"Post-processing: LM={ENABLE_LM_DECODING}, Unicode={ENABLE_UNICODE_NORMALIZATION}, SentenceEnd={ENABLE_SENTENCE_END_CHAR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8710a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processor: arijitx/wav2vec2-xls-r-300m-bengali\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a036f500f64058b70271d020f84707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7d81ffd2ea4650acfa0a394272fc6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679678d51bb349ca89a5e8f454af8206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2132fe8d70745f993e34a057a26c1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79bf06dd3fb048eba43b52696d418bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/309 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: arijitx/wav2vec2-xls-r-300m-bengali\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697722720a5a42a9a64b9b43cb46dcc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a40a2d4d71488d9b4df3bb8399c660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b935d19c3a487ebe3a438d68e4914b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading processor: {MODEL_NAME}\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_NAME)\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14217f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpectralGatingDenoiser class defined\n"
     ]
    }
   ],
   "source": [
    "class SpectralGatingDenoiser:\n",
    "    \"\"\"\n",
    "    Conservative spectral gating denoiser optimized for ASR (not audio quality).\n",
    "    \n",
    "    Uses soft masking with no hard zeroing to preserve phonetic content.\n",
    "    Designed for Bangla speech where consonant articulation is critical.\n",
    "    \n",
    "    Args:\n",
    "        sample_rate: Audio sample rate (default: 16000)\n",
    "        threshold_k: Noise gate threshold multiplier (default: 2.0)\n",
    "                     Higher = more conservative (less denoising)\n",
    "        win_length_ms: STFT window length in milliseconds\n",
    "        hop_length_ms: STFT hop length in milliseconds\n",
    "        soft_mask_min: Minimum mask value to prevent hard zeroing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_rate=16000,\n",
    "        threshold_k=2.0,\n",
    "        win_length_ms=25,\n",
    "        hop_length_ms=10,\n",
    "        soft_mask_min=0.1\n",
    "    ):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.threshold_k = threshold_k\n",
    "        self.soft_mask_min = soft_mask_min\n",
    "        \n",
    "        # Convert ms to samples\n",
    "        self.win_length = int(win_length_ms * sample_rate / 1000)\n",
    "        self.hop_length = int(hop_length_ms * sample_rate / 1000)\n",
    "        \n",
    "        # Ensure win_length is valid\n",
    "        if self.win_length % 2 == 1:\n",
    "            self.win_length += 1\n",
    "            \n",
    "    def __call__(self, waveform):\n",
    "        \"\"\"\n",
    "        Apply spectral gating to waveform.\n",
    "        \n",
    "        Args:\n",
    "            waveform: numpy array or torch tensor of shape (n_samples,)\n",
    "            \n",
    "        Returns:\n",
    "            Denoised waveform as numpy array\n",
    "        \"\"\"\n",
    "        # Convert to numpy if tensor\n",
    "        if isinstance(waveform, torch.Tensor):\n",
    "            waveform = waveform.cpu().numpy()\n",
    "            \n",
    "        # Ensure mono\n",
    "        if waveform.ndim > 1:\n",
    "            waveform = waveform.mean(axis=0)\n",
    "            \n",
    "        # Skip if audio too short\n",
    "        if len(waveform) < self.win_length:\n",
    "            return waveform\n",
    "            \n",
    "        # Step 1: Compute STFT with Hann window\n",
    "        stft = librosa.stft(\n",
    "            waveform,\n",
    "            n_fft=self.win_length,\n",
    "            hop_length=self.hop_length,\n",
    "            window='hann'\n",
    "        )\n",
    "        \n",
    "        magnitude = np.abs(stft)\n",
    "        phase = np.angle(stft)\n",
    "        \n",
    "        # Step 2: Estimate noise profile from low-energy frames\n",
    "        # Use bottom 20% of frames by energy as noise estimate\n",
    "        frame_energy = np.sum(magnitude ** 2, axis=0)\n",
    "        noise_threshold_percentile = 20\n",
    "        noise_frames_mask = frame_energy <= np.percentile(frame_energy, noise_threshold_percentile)\n",
    "        \n",
    "        # Ensure we have some noise frames\n",
    "        if noise_frames_mask.sum() < 5:\n",
    "            # Fallback: use lowest 5 frames\n",
    "            noise_frame_indices = np.argsort(frame_energy)[:5]\n",
    "            noise_frames_mask = np.zeros_like(noise_frames_mask, dtype=bool)\n",
    "            noise_frames_mask[noise_frame_indices] = True\n",
    "            \n",
    "        noise_magnitude = magnitude[:, noise_frames_mask]\n",
    "        \n",
    "        # Step 3: Compute per-frequency noise statistics\n",
    "        noise_mean = np.mean(noise_magnitude, axis=1, keepdims=True)\n",
    "        noise_std = np.std(noise_magnitude, axis=1, keepdims=True)\n",
    "        \n",
    "        # Add small epsilon to prevent division by zero\n",
    "        noise_std = np.maximum(noise_std, 1e-8)\n",
    "        \n",
    "        # Step 4: Compute gating threshold\n",
    "        # threshold(f) = noise_mean(f) + k * noise_std(f)\n",
    "        threshold = noise_mean + self.threshold_k * noise_std\n",
    "        \n",
    "        # Step 5: Apply soft spectral mask\n",
    "        # Mask = min(1.0, max(soft_mask_min, magnitude / threshold))\n",
    "        mask = magnitude / (threshold + 1e-8)\n",
    "        mask = np.clip(mask, self.soft_mask_min, 1.0)\n",
    "        \n",
    "        # Apply mask to magnitude\n",
    "        denoised_magnitude = magnitude * mask\n",
    "        \n",
    "        # Step 6: Reconstruct with original phase\n",
    "        denoised_stft = denoised_magnitude * np.exp(1j * phase)\n",
    "        \n",
    "        # Inverse STFT\n",
    "        denoised_waveform = librosa.istft(\n",
    "            denoised_stft,\n",
    "            hop_length=self.hop_length,\n",
    "            window='hann',\n",
    "            length=len(waveform)  # Ensure same length as input\n",
    "        )\n",
    "        \n",
    "        return denoised_waveform\n",
    "\n",
    "\n",
    "print(\"SpectralGatingDenoiser class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9def269a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASRDataset and collate_fn defined\n"
     ]
    }
   ],
   "source": [
    "class ASRDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Memory-safe dataset for long-form ASR with overlap-aware chunking and on-the-fly denoising.\n",
    "    \n",
    "    Features:\n",
    "    - Lazy audio loading (no preloading)\n",
    "    - Overlap-aware chunk-wise processing\n",
    "    - Optional spectral gating denoising\n",
    "    - No intermediate file I/O\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        audio_paths,\n",
    "        chunk_length_sec=15,\n",
    "        overlap_sec=3,\n",
    "        sample_rate=16000,\n",
    "        denoiser=None\n",
    "    ):\n",
    "        self.audio_paths = audio_paths\n",
    "        self.chunk_length_sec = chunk_length_sec\n",
    "        self.overlap_sec = overlap_sec\n",
    "        self.sample_rate = sample_rate\n",
    "        self.denoiser = denoiser\n",
    "        \n",
    "        # Calculate step size (chunk length minus overlap)\n",
    "        self.step_sec = chunk_length_sec - overlap_sec\n",
    "        \n",
    "        # Precompute chunk indices for each audio file\n",
    "        self.chunk_info = []  # [(audio_idx, chunk_idx, start_sec, end_sec, total_chunks)]\n",
    "        \n",
    "        for audio_idx, audio_path in enumerate(audio_paths):\n",
    "            # Get audio duration without loading full file\n",
    "            info = torchaudio.info(audio_path)\n",
    "            duration_sec = info.num_frames / info.sample_rate\n",
    "            \n",
    "            # Calculate chunks with overlap\n",
    "            chunk_idx = 0\n",
    "            start_sec = 0.0\n",
    "            \n",
    "            while start_sec < duration_sec:\n",
    "                end_sec = min(start_sec + chunk_length_sec, duration_sec)\n",
    "                self.chunk_info.append((audio_idx, chunk_idx, start_sec, end_sec))\n",
    "                start_sec += self.step_sec\n",
    "                chunk_idx += 1\n",
    "            \n",
    "            # Store total chunks for this audio (update all entries)\n",
    "            total_chunks = chunk_idx\n",
    "            for i in range(len(self.chunk_info) - total_chunks, len(self.chunk_info)):\n",
    "                audio_idx_stored, chunk_idx_stored, start_stored, end_stored = self.chunk_info[i]\n",
    "                self.chunk_info[i] = (audio_idx_stored, chunk_idx_stored, start_stored, end_stored, total_chunks)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.chunk_info)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load and process a single chunk with overlap information.\n",
    "        \n",
    "        Returns:\n",
    "            dict with:\n",
    "                - waveform: denoised audio chunk tensor\n",
    "                - audio_path: original audio file path\n",
    "                - chunk_idx: chunk index\n",
    "                - total_chunks: total chunks for this audio\n",
    "                - start_sec: chunk start time in seconds\n",
    "                - end_sec: chunk end time in seconds\n",
    "        \"\"\"\n",
    "        audio_idx, chunk_idx, start_sec, end_sec, total_chunks = self.chunk_info[idx]\n",
    "        audio_path = self.audio_paths[audio_idx]\n",
    "        \n",
    "        # Load audio chunk on-the-fly\n",
    "        start_frame = int(start_sec * self.sample_rate)\n",
    "        num_frames = int((end_sec - start_sec) * self.sample_rate)\n",
    "        \n",
    "        # Load only the required chunk\n",
    "        waveform, sr = torchaudio.load(\n",
    "            audio_path,\n",
    "            frame_offset=start_frame,\n",
    "            num_frames=num_frames\n",
    "        )\n",
    "        \n",
    "        # Convert to mono and resample if needed\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "            \n",
    "        if sr != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "            \n",
    "        # Normalize to [-1, 1]\n",
    "        max_val = torch.max(torch.abs(waveform))\n",
    "        if max_val > 0:\n",
    "            waveform = waveform / max_val\n",
    "            \n",
    "        # Apply denoising if enabled\n",
    "        if self.denoiser is not None:\n",
    "            # Denoiser works on 1D numpy/tensor\n",
    "            waveform_np = waveform.squeeze(0).numpy()\n",
    "            denoised_np = self.denoiser(waveform_np)\n",
    "            waveform = torch.from_numpy(denoised_np).unsqueeze(0)\n",
    "            \n",
    "        # Skip if chunk too short (< 0.5 sec)\n",
    "        if waveform.shape[1] < self.sample_rate // 2:\n",
    "            # Return empty marker\n",
    "            return {\n",
    "                'waveform': None,\n",
    "                'audio_path': audio_path,\n",
    "                'chunk_idx': chunk_idx,\n",
    "                'total_chunks': total_chunks,\n",
    "                'start_sec': start_sec,\n",
    "                'end_sec': end_sec\n",
    "            }\n",
    "            \n",
    "        return {\n",
    "            'waveform': waveform.squeeze(0),  # Return 1D tensor\n",
    "            'audio_path': audio_path,\n",
    "            'chunk_idx': chunk_idx,\n",
    "            'total_chunks': total_chunks,\n",
    "            'start_sec': start_sec,\n",
    "            'end_sec': end_sec\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle variable-length chunks.\"\"\"\n",
    "    # Filter out None waveforms\n",
    "    valid_batch = [item for item in batch if item['waveform'] is not None]\n",
    "    \n",
    "    if len(valid_batch) == 0:\n",
    "        return None\n",
    "        \n",
    "    return valid_batch\n",
    "\n",
    "\n",
    "print(\"ASRDataset with overlap-aware chunking and collate_fn defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1174cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def transcribe_batch(batch, processor, model, device):\n",
    "    \"\"\"\n",
    "    Transcribe a batch of audio chunks.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of chunk dictionaries\n",
    "        processor: Wav2Vec2Processor\n",
    "        model: Wav2Vec2ForCTC model\n",
    "        device: torch device\n",
    "        \n",
    "    Returns:\n",
    "        List of transcriptions\n",
    "    \"\"\"\n",
    "    if batch is None or len(batch) == 0:\n",
    "        return []\n",
    "        \n",
    "    # Extract waveforms\n",
    "    waveforms = [item['waveform'].numpy() for item in batch]\n",
    "    \n",
    "    # Process batch\n",
    "    inputs = processor(\n",
    "        waveforms,\n",
    "        sampling_rate=SAMPLE_RATE,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    input_values = inputs.input_values.to(device)\n",
    "    \n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "        \n",
    "    # Greedy CTC decoding\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcriptions = processor.batch_decode(predicted_ids)\n",
    "    \n",
    "    return transcriptions\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Minimal text cleanup: remove extra spaces and empty tokens.\"\"\"\n",
    "    # Remove special tokens that might appear\n",
    "    text = text.replace(\"<s>\", \"\").replace(\"</s>\", \"\")\n",
    "    text = text.replace(\"<pad>\", \"\").replace(\"<unk>\", \"\")\n",
    "    # Normalize whitespace\n",
    "    text = \" \".join(text.split())\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def find_overlap_match(text1, text2, min_overlap_words=2, max_overlap_words=15):\n",
    "    \"\"\"\n",
    "    Find the best overlap between the end of text1 and beginning of text2.\n",
    "    \n",
    "    Uses longest common subsequence matching to handle slight variations\n",
    "    in transcription at chunk boundaries.\n",
    "    \n",
    "    Args:\n",
    "        text1: First transcription (previous chunk)\n",
    "        text2: Second transcription (current chunk)\n",
    "        min_overlap_words: Minimum words to consider for overlap\n",
    "        max_overlap_words: Maximum words to look back/forward for overlap\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (overlap_end_idx_in_text1, overlap_start_idx_in_text2, confidence)\n",
    "    \"\"\"\n",
    "    words1 = text1.split()\n",
    "    words2 = text2.split()\n",
    "    \n",
    "    if len(words1) < min_overlap_words or len(words2) < min_overlap_words:\n",
    "        return len(words1), 0, 0.0\n",
    "    \n",
    "    # Look at the end of text1 and beginning of text2\n",
    "    end_words1 = words1[-max_overlap_words:] if len(words1) > max_overlap_words else words1\n",
    "    start_words2 = words2[:max_overlap_words] if len(words2) > max_overlap_words else words2\n",
    "    \n",
    "    best_overlap = 0\n",
    "    best_confidence = 0.0\n",
    "    best_i = len(end_words1)\n",
    "    best_j = 0\n",
    "    \n",
    "    # Try different overlap lengths\n",
    "    for overlap_len in range(min_overlap_words, min(len(end_words1), len(start_words2)) + 1):\n",
    "        # Get candidate overlap regions\n",
    "        end_region = \" \".join(end_words1[-overlap_len:])\n",
    "        start_region = \" \".join(start_words2[:overlap_len])\n",
    "        \n",
    "        # Calculate similarity using SequenceMatcher\n",
    "        similarity = SequenceMatcher(None, end_region, start_region).ratio()\n",
    "        \n",
    "        # Weight longer overlaps slightly more\n",
    "        weighted_score = similarity * (1 + overlap_len * 0.05)\n",
    "        \n",
    "        if similarity > 0.6 and weighted_score > best_confidence:\n",
    "            best_overlap = overlap_len\n",
    "            best_confidence = weighted_score\n",
    "            best_i = len(words1) - overlap_len\n",
    "            best_j = overlap_len\n",
    "    \n",
    "    # If no good overlap found, try word-by-word exact matching\n",
    "    if best_confidence < 0.6:\n",
    "        for i in range(max(0, len(end_words1) - max_overlap_words), len(end_words1)):\n",
    "            for j in range(min(max_overlap_words, len(start_words2))):\n",
    "                if end_words1[i].lower() == start_words2[j].lower():\n",
    "                    # Found matching word, extend match\n",
    "                    match_len = 1\n",
    "                    while (i + match_len < len(end_words1) and \n",
    "                           j + match_len < len(start_words2) and \n",
    "                           end_words1[i + match_len].lower() == start_words2[j + match_len].lower()):\n",
    "                        match_len += 1\n",
    "                    \n",
    "                    if match_len >= min_overlap_words:\n",
    "                        # Calculate position in original words1\n",
    "                        original_i = len(words1) - len(end_words1) + i\n",
    "                        return original_i, j + match_len, 0.8\n",
    "    \n",
    "    if best_confidence > 0.6:\n",
    "        # Calculate position in original words1\n",
    "        original_i = len(words1) - len(end_words1) + (len(end_words1) - best_overlap)\n",
    "        return original_i, best_j, best_confidence\n",
    "    \n",
    "    return len(words1), 0, 0.0\n",
    "\n",
    "\n",
    "def stitch_transcriptions(transcriptions_with_timing):\n",
    "    \"\"\"\n",
    "    Stitch overlapping transcriptions together intelligently.\n",
    "    \n",
    "    Args:\n",
    "        transcriptions_with_timing: List of tuples (chunk_idx, start_sec, end_sec, transcription)\n",
    "        \n",
    "    Returns:\n",
    "        Merged transcription string\n",
    "    \"\"\"\n",
    "    if not transcriptions_with_timing:\n",
    "        return \"\"\n",
    "    \n",
    "    # Sort by chunk index\n",
    "    sorted_transcriptions = sorted(transcriptions_with_timing, key=lambda x: x[0])\n",
    "    \n",
    "    # Clean all transcriptions first\n",
    "    cleaned = [(idx, start, end, clean_text(text)) for idx, start, end, text in sorted_transcriptions]\n",
    "    \n",
    "    # Filter out empty transcriptions\n",
    "    cleaned = [(idx, start, end, text) for idx, start, end, text in cleaned if text.strip()]\n",
    "    \n",
    "    if not cleaned:\n",
    "        return \"\"\n",
    "    \n",
    "    if len(cleaned) == 1:\n",
    "        return cleaned[0][3]\n",
    "    \n",
    "    # Stitch transcriptions with overlap detection\n",
    "    result_words = cleaned[0][3].split()\n",
    "    \n",
    "    for i in range(1, len(cleaned)):\n",
    "        prev_text = \" \".join(result_words)\n",
    "        curr_text = cleaned[i][3]\n",
    "        \n",
    "        # Find overlap between accumulated result and current chunk\n",
    "        overlap_end, overlap_start, confidence = find_overlap_match(prev_text, curr_text)\n",
    "        \n",
    "        curr_words = curr_text.split()\n",
    "        \n",
    "        if confidence > 0.5:\n",
    "            # Good overlap found - merge at overlap point\n",
    "            result_words = result_words[:overlap_end] + curr_words[overlap_start:]\n",
    "        else:\n",
    "            # No clear overlap - just concatenate (may cause some duplication)\n",
    "            # But prefer current chunk's beginning as it's fresher\n",
    "            result_words.extend(curr_words)\n",
    "    \n",
    "    return \" \".join(result_words)\n",
    "\n",
    "\n",
    "print(\"Transcription helper functions with overlap-aware stitching defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ecaf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing Functions\n",
    "\n",
    "def lm_decode_with_ngram(text, lm_path=None, alpha=0.5, beta=1.5):\n",
    "    \"\"\"\n",
    "    Apply n-gram language model decoding to improve transcription.\n",
    "    \n",
    "    This function would use KenLM or pyctcdecode for beam search with LM.\n",
    "    For now, it's a placeholder that returns the text as-is.\n",
    "    \n",
    "    Args:\n",
    "        text: Input transcription text\n",
    "        lm_path: Path to KenLM language model file (.arpa or .bin)\n",
    "        alpha: LM weight\n",
    "        beta: Word insertion bonus\n",
    "        \n",
    "    Returns:\n",
    "        LM-corrected text\n",
    "    \"\"\"\n",
    "    # Placeholder implementation\n",
    "    # To use this, you would need:\n",
    "    # 1. Install: pip install pyctcdecode\n",
    "    # 2. Download/train a Bengali language model\n",
    "    # 3. Use pyctcdecode.BeamSearchDecoderCTC with the model\n",
    "    \n",
    "    # For now, just return original text\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_unicode_bangla(text):\n",
    "    \"\"\"\n",
    "    Normalize Unicode characters for Bengali text.\n",
    "    \n",
    "    Handles:\n",
    "    - NFC/NFD normalization\n",
    "    - Bengali-specific character normalization\n",
    "    - Zero-width joiners and non-joiners\n",
    "    - Common Unicode variants\n",
    "    \n",
    "    Args:\n",
    "        text: Input Bengali text\n",
    "        \n",
    "    Returns:\n",
    "        Normalized text\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    # Step 1: Apply NFC (Canonical Composition) normalization\n",
    "    # This is the standard for Bengali text\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    \n",
    "    # Step 2: Bengali-specific normalizations\n",
    "    replacements = {\n",
    "        # Normalize various forms of zero\n",
    "        '০': '০',  # Ensure Bengali zero\n",
    "        '\\u09e6': '০',  # Alternative Bengali zero\n",
    "        \n",
    "        # Normalize anusvara and chandrabindu variants\n",
    "        '\\u0981': 'ঁ',  # Chandrabindu\n",
    "        '\\u0982': 'ং',  # Anusvara\n",
    "        \n",
    "        # Normalize nukta forms\n",
    "        '\\u09bc': '়',  # Bengali nukta\n",
    "        \n",
    "        # Normalize virama (hasant)\n",
    "        '\\u09cd': '্',  # Hasant/Virama\n",
    "        \n",
    "        # Normalize common punctuation\n",
    "        '|': '।',  # Replace pipe with dari\n",
    "        '॥': '।।',  # Double dari normalization\n",
    "    }\n",
    "    \n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    \n",
    "    # Step 3: Remove zero-width characters (except necessary ones)\n",
    "    # Keep Zero Width Joiner (ZWJ) and Zero Width Non-Joiner (ZWNJ) as they're important for Bengali\n",
    "    # But remove other zero-width spaces\n",
    "    text = re.sub(r'[\\u200b\\u200c\\u200d\\ufeff]', '', text)\n",
    "    \n",
    "    # Step 4: Normalize multiple spaces to single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Step 5: Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def add_sentence_ending(text, end_char=\"।\"):\n",
    "    \"\"\"\n",
    "    Append sentence-ending character if not present.\n",
    "    \n",
    "    For Bengali, the dari (।) is the standard sentence-ending punctuation.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        end_char: Character to append (default: Bengali dari ।)\n",
    "        \n",
    "    Returns:\n",
    "        Text with sentence ending\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    # Check if text already ends with common Bengali punctuation\n",
    "    bengali_punctuation = ['।', '॥', '?', '!', '.']\n",
    "    \n",
    "    if not any(text.endswith(p) for p in bengali_punctuation):\n",
    "        text = text + end_char\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def apply_post_processing(text, enable_lm=False, enable_unicode=True, enable_end_char=True, end_char=\"।\"):\n",
    "    \"\"\"\n",
    "    Apply all post-processing steps to transcription.\n",
    "    \n",
    "    Args:\n",
    "        text: Input transcription\n",
    "        enable_lm: Enable n-gram LM decoding\n",
    "        enable_unicode: Enable Unicode normalization\n",
    "        enable_end_char: Enable sentence ending character\n",
    "        end_char: Sentence ending character\n",
    "        \n",
    "    Returns:\n",
    "        Post-processed text\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    # Step 1: N-gram LM decoding (if enabled and available)\n",
    "    if enable_lm:\n",
    "        text = lm_decode_with_ngram(text)\n",
    "    \n",
    "    # Step 2: Unicode normalization\n",
    "    if enable_unicode:\n",
    "        text = normalize_unicode_bangla(text)\n",
    "    \n",
    "    # Step 3: Add sentence ending\n",
    "    if enable_end_char:\n",
    "        text = add_sentence_ending(text, end_char)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "print(\"Post-processing functions defined (LM decoding, Unicode normalization, sentence ending)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ebc88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denoiser initialized (threshold_k=2.0)\n",
      "\n",
      "Found 24 test audio files\n",
      "Dataset created: 5341 total chunks\n",
      "Processing with batch_size=4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio chunks: 100%|██████████| 1336/1336 [11:41<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 24 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize denoiser (if enabled)\n",
    "denoiser = None\n",
    "if ENABLE_DENOISING:\n",
    "    denoiser = SpectralGatingDenoiser(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        threshold_k=NOISE_GATE_THRESHOLD_K,\n",
    "        win_length_ms=STFT_WIN_LENGTH_MS,\n",
    "        hop_length_ms=STFT_HOP_LENGTH_MS,\n",
    "        soft_mask_min=SOFT_MASK_MIN\n",
    "    )\n",
    "    print(f\"Denoiser initialized (threshold_k={NOISE_GATE_THRESHOLD_K})\")\n",
    "else:\n",
    "    print(\"Denoising disabled\")\n",
    "\n",
    "# Get test audio files\n",
    "test_files = sorted(glob.glob(os.path.join(TEST_AUDIO_DIR, \"audio\", \"*.wav\")))\n",
    "print(f\"\\nFound {len(test_files)} test audio files\")\n",
    "\n",
    "# Create dataset and dataloader with overlap-aware chunking\n",
    "dataset = ASRDataset(\n",
    "    audio_paths=test_files,\n",
    "    chunk_length_sec=CHUNK_LENGTH_SEC,\n",
    "    overlap_sec=OVERLAP_SEC,\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    denoiser=denoiser\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,  # Process 4 chunks at a time\n",
    "    shuffle=False,\n",
    "    num_workers=2,  # Set to 0 for Kaggle compatibility\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Dataset created: {len(dataset)} total chunks (with {OVERLAP_SEC}s overlap)\")\n",
    "print(f\"Processing with batch_size=4\\n\")\n",
    "\n",
    "# Process all chunks and aggregate by file with timing info\n",
    "from collections import defaultdict\n",
    "file_transcriptions = defaultdict(list)\n",
    "\n",
    "for batch in tqdm(dataloader, desc=\"Processing audio chunks\"):\n",
    "    if batch is None:\n",
    "        continue\n",
    "        \n",
    "    # Transcribe batch\n",
    "    transcriptions = transcribe_batch(batch, processor, model, DEVICE)\n",
    "    \n",
    "    # Aggregate transcriptions by file with timing information\n",
    "    for item, transcription in zip(batch, transcriptions):\n",
    "        audio_path = item['audio_path']\n",
    "        filename = os.path.basename(audio_path)\n",
    "        chunk_idx = item['chunk_idx']\n",
    "        start_sec = item['start_sec']\n",
    "        end_sec = item['end_sec']\n",
    "        \n",
    "        if transcription.strip():\n",
    "            file_transcriptions[filename].append((chunk_idx, start_sec, end_sec, transcription))\n",
    "\n",
    "# Merge transcriptions for each file using overlap-aware stitching\n",
    "print(\"\\nApplying overlap-aware stitching and post-processing...\")\n",
    "results = []\n",
    "for filename in sorted(file_transcriptions.keys()):\n",
    "    # Use the stitching function\n",
    "    full_text = stitch_transcriptions(file_transcriptions[filename])\n",
    "    \n",
    "    # Apply post-processing\n",
    "    processed_text = apply_post_processing(\n",
    "        full_text,\n",
    "        enable_lm=ENABLE_LM_DECODING,\n",
    "        enable_unicode=ENABLE_UNICODE_NORMALIZATION,\n",
    "        enable_end_char=ENABLE_SENTENCE_END_CHAR,\n",
    "        end_char=SENTENCE_END_CHAR\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        \"filename\": filename,\n",
    "        \"transcript\": processed_text\n",
    "    })\n",
    "\n",
    "print(f\"Processed {len(results)} files with overlap-aware stitching + post-processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32719368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to: /kaggle/working/submission.csv\n",
      "\n",
      "Submission preview (24 rows):\n",
      "       filename                                         transcript\n",
      "0  test_001.wav  এআক্সক্ষির এটেনিকেপকা আপনাকেদে ালোমানার মিডিগি...\n",
      "1  test_002.wav  মিন্তু রচ্ছাধারীনা আগিন সৈনাআমি সব দিল পমলা এব...\n",
      "2  test_003.wav  গল্পুটির সত্য আনন্দ পাবলিশাস প্রাইভেটলিমেটে কো...\n",
      "3  test_004.wav  যে কোনো জায়গায় যেতে রাতের ট্রেনি আমাদের প্সব...\n",
      "4  test_005.wav  বেচি নিবেদন ফ্রাইডেইক্লাসেক্স পরে বকিমেগ গাকছে...\n",
      "5  test_006.wav  আাদের খুব প্রন্দ হইছে আা ছেলে পছন্দমি ইলে আপনা...\n",
      "6  test_008.wav  বাদরির পোচাগর এই রকম াঙি দুগডবাটা পড সইে যাওয়...\n",
      "7  test_009.wav  দ এব দি কেকদ মৃত্যাগত গলতে সরায কেবে িতঅমিবভাজ...\n",
      "8  test_010.wav  আাই একটা তাড়াতালি ক ইে ফোন জলে আসছে আরে এতারা...\n",
      "9  test_011.wav  বেচি নিবেদন ফ্রাইডে ক্লাসেক্স ই জুই মন্টু গিয়...\n"
     ]
    }
   ],
   "source": [
    "SUBMISSION_PATH = \"/kaggle/working/\"\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame(results)\n",
    "submission_df = submission_df[[\"filename\", \"transcript\"]]\n",
    "\n",
    "# Fill any empty transcriptions\n",
    "submission_df[\"transcript\"] = submission_df[\"transcript\"].fillna(\"\")\n",
    "\n",
    "# Save submission\n",
    "submission_df.to_csv(SUBMISSION_PATH + \"submission.csv\", index=False, encoding=\"utf-8\")\n",
    "print(f\"Submission saved to: {SUBMISSION_PATH}submission.csv\")\n",
    "\n",
    "# Display preview\n",
    "print(f\"\\nSubmission preview ({len(submission_df)} rows):\")\n",
    "print(submission_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d99a153",
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/kaggle/working/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_102/1035954836.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Verify submission file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfinal_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSUBMISSION_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Submission verification:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  - Total rows: {len(final_df)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  - Columns: {list(final_df.columns)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/kaggle/working/'"
     ]
    }
   ],
   "source": [
    "# Verify submission file\n",
    "final_df = pd.read_csv(SUBMISSION_PATH)\n",
    "print(\"Submission verification:\")\n",
    "print(f\"  - Total rows: {len(final_df)}\")\n",
    "print(f\"  - Columns: {list(final_df.columns)}\")\n",
    "print(f\"  - Empty transcriptions: {(final_df['transcription'] == '').sum()}\")\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
