{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "!pip install --upgrade huggingface_hub transformers librosa torchaudio -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import WhisperTokenizer\n",
    "from transformers import WhisperProcessor\n",
    "from transformers import WhisperFeatureExtractor\n",
    "from transformers import WhisperForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "device_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device used is: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BanglaASR model\n",
    "model_path = \"bangla-speech-processing/BanglaASR\"\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_path)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_path)\n",
    "processor = WhisperProcessor.from_pretrained(model_path)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo_inference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Test inference on a sample audio from HuggingFace\n",
    "mp3_path = \"https://huggingface.co/bangla-speech-processing/BanglaASR/resolve/main/mp3/common_voice_bn_31515636.mp3\"\n",
    "\n",
    "speech_array, sampling_rate = torchaudio.load(mp3_path, format=\"mp3\")\n",
    "speech_array = speech_array[0].numpy()\n",
    "speech_array = librosa.resample(np.asarray(speech_array), orig_sr=sampling_rate, target_sr=16000)\n",
    "input_features = feature_extractor(speech_array, sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "\n",
    "predicted_ids = model.generate(inputs=input_features.to(device))[0]\n",
    "transcription = processor.decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"Demo Transcription Result:\")\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get_audio_files",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test audio directory path\n",
    "test_audio_dir = \"/kaggle/input/dl-sprint-4-0-bengali-long-form-speech-recognition/transcription/transcription/test/audio\"\n",
    "\n",
    "# Get all .wav audio files\n",
    "audio_files = sorted(glob(os.path.join(test_audio_dir, \"*.wav\")))\n",
    "\n",
    "print(f\"Found {len(audio_files)} audio files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chunked_transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SAMPLING_RATE = 16000  # Whisper expects 16kHz\n",
    "CHUNK_DURATION = 30    # seconds per chunk\n",
    "\n",
    "def load_audio(audio_path):\n",
    "    \"\"\"\n",
    "    Load audio file with proper preprocessing for Whisper.\n",
    "    - Resample to 16kHz (Whisper requirement)\n",
    "    - Convert stereo to mono\n",
    "    \"\"\"\n",
    "    # Load with librosa, automatically resamples to target sr\n",
    "    # mono=True ensures single channel output\n",
    "    audio, sr = librosa.load(audio_path, sr=SAMPLING_RATE, mono=True)\n",
    "    return audio\n",
    "\n",
    "def transcribe_audio_chunk(audio_chunk):\n",
    "    \"\"\"\n",
    "    Transcribe a single audio chunk using BanglaASR model.\n",
    "    \n",
    "    Args:\n",
    "        audio_chunk: numpy array of audio samples at 16kHz\n",
    "    \n",
    "    Returns:\n",
    "        Transcript string\n",
    "    \"\"\"\n",
    "    input_features = feature_extractor(\n",
    "        audio_chunk, \n",
    "        sampling_rate=SAMPLING_RATE, \n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features\n",
    "    \n",
    "    predicted_ids = model.generate(inputs=input_features.to(device))[0]\n",
    "    transcription = processor.decode(predicted_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return transcription.strip()\n",
    "\n",
    "def transcribe_long_audio(audio_path, chunk_duration=30):\n",
    "    \"\"\"\n",
    "    Transcribe long audio by splitting into chunks.\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Path to the audio file\n",
    "        chunk_duration: Duration of each chunk in seconds\n",
    "    \n",
    "    Returns:\n",
    "        Full transcript as a single string\n",
    "    \"\"\"\n",
    "    # Load and preprocess audio\n",
    "    audio = load_audio(audio_path)\n",
    "    \n",
    "    total_samples = len(audio)\n",
    "    chunk_samples = SAMPLING_RATE * chunk_duration\n",
    "    \n",
    "    transcripts = []\n",
    "    num_chunks = (total_samples + chunk_samples - 1) // chunk_samples\n",
    "    \n",
    "    # Process audio in chunks\n",
    "    for i, start in enumerate(range(0, total_samples, chunk_samples)):\n",
    "        end = min(start + chunk_samples, total_samples)\n",
    "        audio_chunk = audio[start:end]\n",
    "        \n",
    "        # Skip if chunk is too short (less than 0.5 seconds)\n",
    "        if len(audio_chunk) < SAMPLING_RATE * 0.5:\n",
    "            continue\n",
    "        \n",
    "        # Transcribe chunk using BanglaASR\n",
    "        transcript = transcribe_audio_chunk(audio_chunk)\n",
    "        transcripts.append(transcript)\n",
    "    \n",
    "    # Join all chunk transcripts\n",
    "    return \" \".join(transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_single_file",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on first audio file before running on all\n",
    "if len(audio_files) > 0:\n",
    "    test_file = audio_files[0]\n",
    "    filename = os.path.splitext(os.path.basename(test_file))[0]\n",
    "    duration = librosa.get_duration(path=test_file)\n",
    "    \n",
    "    print(f\"Testing on: {filename} ({duration/60:.1f} mins)\")\n",
    "    test_transcript = transcribe_long_audio(test_file, chunk_duration=CHUNK_DURATION)\n",
    "    print(f\"\\nTranscript Preview:\")\n",
    "    print(test_transcript[:500] + \"...\" if len(test_transcript) > 500 else test_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_transcripts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on all audio files\n",
    "results = []\n",
    "\n",
    "for audio_path in tqdm(audio_files, desc=\"Transcribing files\"):\n",
    "    # Extract filename without extension (e.g., \"test_001\" from \"test_001.wav\")\n",
    "    filename = os.path.splitext(os.path.basename(audio_path))[0]\n",
    "    \n",
    "    try:\n",
    "        # Get audio duration for progress info\n",
    "        duration = librosa.get_duration(path=audio_path)\n",
    "        print(f\"\\nProcessing {filename} ({duration/60:.1f} mins)...\")\n",
    "        \n",
    "        transcript = transcribe_long_audio(audio_path, chunk_duration=CHUNK_DURATION)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")\n",
    "        transcript = \"\"\n",
    "    \n",
    "    results.append({\n",
    "        \"filename\": filename,\n",
    "        \"transcript\": transcript\n",
    "    })\n",
    "\n",
    "print(f\"\\nTranscribed {len(results)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_csv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame and save to CSV\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = \"submission.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Saved {len(df)} transcriptions to {output_path}\")\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
