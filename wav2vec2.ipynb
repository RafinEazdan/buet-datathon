{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "!pip install --upgrade huggingface_hub transformers librosa torchaudio noisereduce scipy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import noisereduce as nr\n",
    "from scipy import signal\n",
    "\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d06e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MEMORY OPTIMIZATION SETTINGS\n",
    "# =============================================================================\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Limit CPU threads to reduce memory overhead\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "\n",
    "# Force garbage collection\n",
    "def clear_memory():\n",
    "    \"\"\"Aggressively clear memory\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "print(\"Memory optimization settings applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "device_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device used is: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Wav2Vec2-XLS-R-300M-Bengali model with memory optimizations\n",
    "model_path = \"arijitx/wav2vec2-xls-r-300m-bengali\"\n",
    "\n",
    "print(\"Loading processor...\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_path)\n",
    "\n",
    "print(\"Loading model with memory optimizations...\")\n",
    "# Load model with low_cpu_mem_usage to reduce memory during loading\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    model_path,\n",
    "    low_cpu_mem_usage=True,  # Reduces peak memory during loading\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32  # Use FP16 on GPU\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Set to evaluation mode (disables dropout, saves memory)\n",
    "model.eval()\n",
    "\n",
    "# Clear any loading artifacts\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"‚úÖ Model loaded on {device}\")\n",
    "print(f\"   Model dtype: {next(model.parameters()).dtype}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU Memory allocated: {torch.cuda.memory_allocated() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get_audio_files",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test audio directory path\n",
    "test_audio_dir = \"/kaggle/input/dl-sprint-4-0-bengali-long-form-speech-recognition/transcription/transcription/test/audio\"\n",
    "\n",
    "# Get all .wav audio files\n",
    "audio_files = sorted(glob(os.path.join(test_audio_dir, \"*.wav\")))\n",
    "\n",
    "print(f\"Found {len(audio_files)} audio files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "audio_preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SIMPLE MEMORY-EFFICIENT AUDIO PROCESSING - NO MS-LEVEL PREPROCESSING\n",
    "# =============================================================================\n",
    "# \n",
    "# New Approach: Minimal preprocessing, maximum memory efficiency\n",
    "# - Skip complex noise reduction (memory intensive)\n",
    "# - Skip frame-level processing (not needed for wav2vec2)\n",
    "# - Process in 30-second chunks at a time\n",
    "# - Simple: load ‚Üí normalize ‚Üí transcribe ‚Üí clear ‚Üí repeat\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "SAMPLING_RATE = 16000  # Wav2Vec2 requirement\n",
    "\n",
    "# Simple chunk processing at SECOND level (not ms)\n",
    "CHUNK_DURATION_SECONDS = 30  # Process 30 seconds at a time\n",
    "OVERLAP_SECONDS = 2          # Small overlap to avoid cutting words\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SIMPLE MEMORY-EFFICIENT APPROACH\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Processing in {CHUNK_DURATION_SECONDS}-second chunks\")\n",
    "print(f\"No complex preprocessing - just normalize and transcribe\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chunked_transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STREAMLINED TRANSCRIPTION - PROCESS ONE CHUNK AT A TIME\n",
    "# =============================================================================\n",
    "\n",
    "def transcribe_chunk_simple(audio_chunk):\n",
    "    \"\"\"\n",
    "    Transcribe one chunk with minimal memory footprint.\n",
    "    No preprocessing - wav2vec2 is robust enough.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Simple normalization only\n",
    "        max_val = np.max(np.abs(audio_chunk))\n",
    "        if max_val > 0:\n",
    "            audio_chunk = audio_chunk / max_val\n",
    "        \n",
    "        # Process with wav2vec2\n",
    "        inputs = processor(\n",
    "            audio_chunk,\n",
    "            sampling_rate=SAMPLING_RATE,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        input_values = inputs.input_values.to(device)\n",
    "        \n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_values).logits\n",
    "            predicted_ids = torch.argmax(logits, dim=-1)\n",
    "            transcript = processor.batch_decode(predicted_ids)[0]\n",
    "        \n",
    "        # Clear memory immediately\n",
    "        del inputs, input_values, logits, predicted_ids\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return transcript.strip()\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            print(f\"    ‚ö†Ô∏è OOM error - skipping chunk\")\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            return \"\"\n",
    "        raise e\n",
    "\n",
    "def transcribe_audio_streaming(audio_path):\n",
    "    \"\"\"\n",
    "    Stream-process audio in chunks to minimize memory usage.\n",
    "    \n",
    "    Process:\n",
    "    1. Get audio duration without loading full file\n",
    "    2. Load ONLY current chunk\n",
    "    3. Transcribe chunk\n",
    "    4. Clear memory\n",
    "    5. Move to next chunk\n",
    "    \n",
    "    This way we never load the full audio into memory.\n",
    "    \"\"\"\n",
    "    # Get total duration without loading audio\n",
    "    duration = librosa.get_duration(path=audio_path)\n",
    "    \n",
    "    chunk_duration = CHUNK_DURATION_SECONDS\n",
    "    overlap = OVERLAP_SECONDS\n",
    "    \n",
    "    transcripts = []\n",
    "    current_time = 0\n",
    "    chunk_num = 0\n",
    "    total_chunks = int(np.ceil(duration / (chunk_duration - overlap)))\n",
    "    \n",
    "    print(f\"  Duration: {duration:.1f}s ‚Üí {total_chunks} chunks of {chunk_duration}s\")\n",
    "    \n",
    "    while current_time < duration:\n",
    "        chunk_num += 1\n",
    "        \n",
    "        # Calculate chunk boundaries\n",
    "        start_time = max(0, current_time - overlap if current_time > 0 else 0)\n",
    "        end_time = min(current_time + chunk_duration, duration)\n",
    "        \n",
    "        # Load ONLY this chunk (memory efficient!)\n",
    "        audio_chunk, sr = librosa.load(\n",
    "            audio_path,\n",
    "            sr=SAMPLING_RATE,\n",
    "            offset=start_time,\n",
    "            duration=end_time - start_time,\n",
    "            mono=True\n",
    "        )\n",
    "        \n",
    "        # Transcribe this chunk\n",
    "        transcript = transcribe_chunk_simple(audio_chunk)\n",
    "        \n",
    "        if transcript:\n",
    "            transcripts.append(transcript)\n",
    "        \n",
    "        # Free chunk memory immediately\n",
    "        del audio_chunk\n",
    "        gc.collect()\n",
    "        \n",
    "        # Move to next chunk\n",
    "        current_time = end_time\n",
    "        \n",
    "        # Progress indicator\n",
    "        if chunk_num % 5 == 0 or chunk_num == total_chunks:\n",
    "            print(f\"    ‚Üí Processed {chunk_num}/{total_chunks} chunks\")\n",
    "    \n",
    "    # Merge transcripts\n",
    "    final_transcript = \" \".join(transcripts)\n",
    "    \n",
    "    # Clear transcripts list\n",
    "    del transcripts\n",
    "    gc.collect()\n",
    "    \n",
    "    return final_transcript\n",
    "\n",
    "print(\"‚úÖ Simple streaming transcription ready\")\n",
    "print(\"   ‚Ä¢ Loads one chunk at a time\")\n",
    "print(\"   ‚Ä¢ No complex preprocessing\")\n",
    "print(\"   ‚Ä¢ Aggressive memory cleanup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_single_file",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the streaming approach on first file\n",
    "\n",
    "if len(audio_files) > 0:\n",
    "    test_file = audio_files[0]\n",
    "    filename = os.path.splitext(os.path.basename(test_file))[0]\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"TESTING STREAMING TRANSCRIPTION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nüìÅ File: {filename}\")\n",
    "    \n",
    "    # Clear memory before starting\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\nüîÑ Processing...\")\n",
    "    transcript = transcribe_audio_streaming(test_file)\n",
    "    \n",
    "    # Clear memory after\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Done!\")\n",
    "    print(f\"üìä Length: {len(transcript)} chars, {len(transcript.split())} words\")\n",
    "    print(f\"\\n{'‚îÄ' * 60}\")\n",
    "    print(\"TRANSCRIPT:\")\n",
    "    print('‚îÄ' * 60)\n",
    "    print(transcript[:300] + \"...\" if len(transcript) > 300 else transcript)\n",
    "    print('‚îÄ' * 60)\n",
    "else:\n",
    "    print(\"‚ùå No audio files found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdf0275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(test_transcript, columns=[\"transcript\"])\n",
    "df.to_csv(\"test_transcript.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_transcripts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all files with streaming approach\n",
    "results = []\n",
    "\n",
    "print(f\"\\nProcessing {len(audio_files)} files with streaming approach...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, audio_path in enumerate(audio_files):\n",
    "    filename = os.path.splitext(os.path.basename(audio_path))[0]\n",
    "    \n",
    "    print(f\"\\n[{i+1}/{len(audio_files)}] {filename}\")\n",
    "    \n",
    "    # Clear memory before each file\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    try:\n",
    "        transcript = transcribe_audio_streaming(audio_path)\n",
    "        print(f\"  ‚úÖ {len(transcript)} chars, {len(transcript.split())} words\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error: {e}\")\n",
    "        transcript = \"\"\n",
    "    \n",
    "    results.append({\n",
    "        \"filename\": filename,\n",
    "        \"transcript\": transcript\n",
    "    })\n",
    "    \n",
    "    # Clear memory after each file\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"‚úÖ Completed {len(results)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_csv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame and save to CSV\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = \"submission.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Saved {len(df)} transcriptions to {output_path}\")\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
