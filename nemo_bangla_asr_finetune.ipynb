{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ NeMo Bangla ASR Fine-tuning Pipeline\n",
    "\n",
    "**Fine-tuning pretrained Bangla Conformer ASR model on large audio dataset**\n",
    "\n",
    "- **Model**: `hishab/titu_stt_bn_conformer_large`\n",
    "- **Framework**: NVIDIA NeMo + PyTorch Lightning\n",
    "- **Hardware**: Kaggle P100 GPU (16GB VRAM)\n",
    "- **Audio**: 40+ minute files ‚Üí 30-second chunks (on-the-fly, no file saving)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Installation\n",
    "\n",
    "Install NeMo toolkit and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install NeMo and dependencies\n",
    "!pip install -q nemo_toolkit['all']\n",
    "!pip install -q soundfile librosa\n",
    "\n",
    "# Verify installation\n",
    "import nemo\n",
    "import nemo.collections.asr as nemo_asr\n",
    "print(f\"NeMo version: {nemo.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import gc\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Tuple, Dict\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "\n",
    "print(\"‚úÖ All imports successful\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üíæ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# ‚ö†Ô∏è UPDATE THESE PATHS FOR YOUR KAGGLE ENVIRONMENT\n",
    "\n",
    "# Input paths (Kaggle dataset)\n",
    "AUDIO_DIR = \"/kaggle/input/your-dataset/audio\"  # Directory containing long audio files\n",
    "TRANSCRIPT_DIR = \"/kaggle/input/your-dataset/transcripts\"  # Directory containing .txt transcript files\n",
    "TEST_AUDIO_DIR = \"/kaggle/input/your-dataset/test\"  # Directory containing test audio files\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = \"/kaggle/working/processed_data\"\n",
    "MANIFEST_DIR = os.path.join(OUTPUT_DIR, \"manifests\")\n",
    "CHECKPOINT_DIR = \"/kaggle/working/checkpoints\"\n",
    "FINAL_MODEL_PATH = \"/kaggle/working/nemo_bangla_asr_finetuned.nemo\"\n",
    "SUBMISSION_PATH = \"/kaggle/working/\"\n",
    "\n",
    "# Audio parameters\n",
    "CHUNK_DURATION = 30.0  # seconds\n",
    "SAMPLE_RATE = 16000  # Hz\n",
    "\n",
    "# Dataset parameters\n",
    "USE_FIRST_50_PERCENT = True  # Use only first 50% of training data\n",
    "\n",
    "# Training parameters (MEMORY OPTIMIZED for P100 16GB)\n",
    "BATCH_SIZE = 1  # Keep at 1 for memory safety\n",
    "MAX_EPOCHS = 10\n",
    "LEARNING_RATE = 2e-5\n",
    "VAL_SPLIT = 0.1  # 10% validation\n",
    "GRADIENT_ACCUMULATION = 4  # Simulate larger batch size\n",
    "INFERENCE_BATCH_SIZE = 4  # For test inference\n",
    "\n",
    "# Memory optimization settings\n",
    "USE_GRADIENT_CHECKPOINTING = True\n",
    "EMPTY_CACHE_EVERY_N_STEPS = 10\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(MANIFEST_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"üìÅ Directory structure created\")\n",
    "print(f\"  Manifests: {MANIFEST_DIR}\")\n",
    "print(f\"  Checkpoints: {CHECKPOINT_DIR}\")\n",
    "print(f\"  Submission: {SUBMISSION_PATH}\")\n",
    "print(\"\\n‚ö° Using ON-THE-FLY chunking (no intermediate files saved)\")\n",
    "print(f\"üíæ Memory optimizations: Gradient accumulation={GRADIENT_ACCUMULATION}, Checkpointing={USE_GRADIENT_CHECKPOINTING}\")\n",
    "if USE_FIRST_50_PERCENT:\n",
    "    print(f\"üìä Dataset: Using first 50% of training data only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìÅ Expected Dataset Structure\n",
    "\n",
    "```\n",
    "/kaggle/input/your-dataset/\n",
    "‚îú‚îÄ‚îÄ audio/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ audio1.wav\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ audio2.wav\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îú‚îÄ‚îÄ transcripts/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ audio1.txt  ‚Üê Same name as audio file\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ audio2.txt\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ test/\n",
    "    ‚îú‚îÄ‚îÄ test1.wav\n",
    "    ‚îú‚îÄ‚îÄ test2.wav\n",
    "    ‚îî‚îÄ‚îÄ ...\n",
    "```\n",
    "\n",
    "**Important**: Each audio file must have a corresponding .txt file with the SAME filename (except extension)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Dataset Size Configuration\n",
    "\n",
    "**Current setting**: `USE_FIRST_50_PERCENT = True`\n",
    "\n",
    "- ‚úÖ **True**: Uses only the **first 50%** of audio files for training (faster, less memory)\n",
    "- ‚ùå **False**: Uses **all 100%** of audio files for training (full dataset, better accuracy)\n",
    "\n",
    "This is useful for:\n",
    "- Quick testing and iteration with smaller dataset\n",
    "- Reducing training time and memory usage\n",
    "- Prototyping before full training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate paths (optional but recommended)\n",
    "print(\"üîç Validating dataset paths...\\n\")\n",
    "\n",
    "paths_to_check = {\n",
    "    \"Audio Directory\": AUDIO_DIR,\n",
    "    \"Transcript Directory\": TRANSCRIPT_DIR,\n",
    "    \"Test Audio Directory\": TEST_AUDIO_DIR\n",
    "}\n",
    "\n",
    "all_valid = True\n",
    "for name, path in paths_to_check.items():\n",
    "    exists = os.path.exists(path)\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"{status} {name}: {path}\")\n",
    "    if not exists:\n",
    "        all_valid = False\n",
    "        print(f\"   ‚ö†Ô∏è  WARNING: Path does not exist!\")\n",
    "\n",
    "if not all_valid:\n",
    "    print(\"\\n‚ö†Ô∏è  Some paths are invalid. Please update the configuration above.\")\n",
    "    print(\"   The notebook will still run, but you may encounter errors later.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All paths validated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéµ Audio Preprocessing (On-the-Fly)\n",
    "\n",
    "Create chunk metadata WITHOUT saving files. Uses **offset-based** manifest entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunk_metadata(\n",
    "    audio_path: str,\n",
    "    chunk_duration: float = 30.0\n",
    ") -> List[Tuple[str, float, float]]:\n",
    "    \"\"\"\n",
    "    Create metadata for audio chunks WITHOUT saving files.\n",
    "    Uses offset-based approach for on-the-fly loading.\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Path to input audio file\n",
    "        chunk_duration: Duration of each chunk in seconds (default: 30.0)\n",
    "        \n",
    "    Returns:\n",
    "        List of (audio_path, offset, duration) tuples\n",
    "    \"\"\"\n",
    "    # Get audio info without loading entire file\n",
    "    info = sf.info(audio_path)\n",
    "    total_duration = info.duration\n",
    "    \n",
    "    # Calculate number of complete chunks\n",
    "    num_chunks = int(total_duration // chunk_duration)\n",
    "    \n",
    "    chunks_metadata = []\n",
    "    \n",
    "    # Create metadata for each chunk (offset-based)\n",
    "    for i in range(num_chunks):\n",
    "        offset = i * chunk_duration\n",
    "        chunks_metadata.append((audio_path, offset, chunk_duration))\n",
    "    \n",
    "    # Note: We discard the final incomplete chunk (< 30s)\n",
    "    \n",
    "    return chunks_metadata\n",
    "\n",
    "\n",
    "def get_audio_duration(audio_path: str) -> float:\n",
    "    \"\"\"\n",
    "    Get audio file duration in seconds.\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Path to audio file\n",
    "        \n",
    "    Returns:\n",
    "        Duration in seconds\n",
    "    \"\"\"\n",
    "    info = sf.info(audio_path)\n",
    "    return info.duration\n",
    "\n",
    "\n",
    "print(\"‚úÖ Audio chunking functions defined (offset-based, no file saving)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_transcript_into_chunks(\n",
    "    transcript: str,\n",
    "    num_chunks: int\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Split a transcript into roughly equal chunks.\n",
    "    \n",
    "    Args:\n",
    "        transcript: Full transcript text\n",
    "        num_chunks: Number of chunks to split into\n",
    "        \n",
    "    Returns:\n",
    "        List of transcript chunks\n",
    "    \"\"\"\n",
    "    if num_chunks <= 0:\n",
    "        return []\n",
    "    \n",
    "    if num_chunks == 1:\n",
    "        return [transcript]\n",
    "    \n",
    "    # Split by sentences (simple approach using common punctuation)\n",
    "    # This works for Bengali and English\n",
    "    import re\n",
    "    \n",
    "    # Split by sentence-ending punctuation\n",
    "    sentences = re.split(r'[‡•§\\.\\!\\?]+', transcript)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    if len(sentences) == 0:\n",
    "        # If no sentences found, split by words\n",
    "        words = transcript.split()\n",
    "        words_per_chunk = max(1, len(words) // num_chunks)\n",
    "        \n",
    "        chunks = []\n",
    "        for i in range(num_chunks):\n",
    "            start_idx = i * words_per_chunk\n",
    "            end_idx = start_idx + words_per_chunk if i < num_chunks - 1 else len(words)\n",
    "            chunk = ' '.join(words[start_idx:end_idx])\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "        return chunks\n",
    "    \n",
    "    # Distribute sentences across chunks\n",
    "    sentences_per_chunk = max(1, len(sentences) // num_chunks)\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * sentences_per_chunk\n",
    "        end_idx = start_idx + sentences_per_chunk if i < num_chunks - 1 else len(sentences)\n",
    "        chunk = ' '.join(sentences[start_idx:end_idx])\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def process_dataset(\n",
    "    audio_dir: str,\n",
    "    transcript_data: Dict[str, str],\n",
    "    chunk_duration: float = 30.0,\n",
    "    use_first_n_percent: float = 1.0\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process all audio files: create chunk metadata and pair with transcripts.\n",
    "    NO FILES ARE SAVED - uses offset-based approach.\n",
    "    \n",
    "    Args:\n",
    "        audio_dir: Directory containing audio files\n",
    "        transcript_data: Dictionary mapping audio files to FULL transcripts\n",
    "                        Format: {\"filename.wav\": \"full transcript text\", ...}\n",
    "        chunk_duration: Duration of each chunk in seconds\n",
    "        use_first_n_percent: Fraction of data to use (0.5 = first 50%)\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with audio_filepath, offset, duration, text\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    # Get all audio files\n",
    "    audio_files = sorted(Path(audio_dir).glob(\"*.wav\"))\n",
    "    \n",
    "    # Take only first N% if specified\n",
    "    if use_first_n_percent < 1.0:\n",
    "        num_files_to_use = int(len(audio_files) * use_first_n_percent)\n",
    "        audio_files = audio_files[:num_files_to_use]\n",
    "        print(f\"üìä Using first {use_first_n_percent*100:.0f}% of data: {num_files_to_use}/{len(sorted(Path(audio_dir).glob('*.wav')))} files\")\n",
    "    \n",
    "    print(f\"Found {len(audio_files)} audio files to process\")\n",
    "    \n",
    "    for audio_path in tqdm(audio_files, desc=\"Processing audio files\"):\n",
    "        filename = audio_path.name\n",
    "        \n",
    "        # Check if transcript exists\n",
    "        if filename not in transcript_data:\n",
    "            print(f\"‚ö†Ô∏è  Skipping {filename}: no transcript found\")\n",
    "            continue\n",
    "        \n",
    "        # Get full transcript\n",
    "        full_transcript = transcript_data[filename]\n",
    "        \n",
    "        # Create chunk metadata (no file saving!)\n",
    "        chunks_metadata = create_chunk_metadata(\n",
    "            str(audio_path),\n",
    "            chunk_duration\n",
    "        )\n",
    "        \n",
    "        num_chunks = len(chunks_metadata)\n",
    "        \n",
    "        # Split transcript into chunks\n",
    "        transcript_chunks = split_transcript_into_chunks(full_transcript, num_chunks)\n",
    "        \n",
    "        # Pair audio chunks with transcript chunks\n",
    "        for idx, (audio_file, offset, duration) in enumerate(chunks_metadata):\n",
    "            # Check if transcript exists for this chunk\n",
    "            if idx >= len(transcript_chunks):\n",
    "                print(f\"‚ö†Ô∏è  No transcript chunk for audio chunk {idx} of {filename}\")\n",
    "                # Use empty string for remaining chunks\n",
    "                text = \"\"\n",
    "            else:\n",
    "                text = transcript_chunks[idx]\n",
    "            \n",
    "            # Validate transcript\n",
    "            if not text or not text.strip():\n",
    "                print(f\"‚ö†Ô∏è  Empty transcript for chunk {idx} of {filename}\")\n",
    "                continue\n",
    "            \n",
    "            # Add to dataset with offset and duration\n",
    "            all_data.append({\n",
    "                \"audio_filepath\": audio_file,\n",
    "                \"offset\": offset,\n",
    "                \"duration\": duration,\n",
    "                \"text\": text.strip()\n",
    "            })\n",
    "    \n",
    "    print(f\"\\n‚úÖ Processed {len(all_data)} valid audio-transcript pairs\")\n",
    "    print(f\"üíæ Space saved: No chunk files created!\")\n",
    "    return all_data\n",
    "\n",
    "\n",
    "print(\"‚úÖ Dataset processing function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Transcripts from .txt Files\n",
    "\n",
    "**Format**: Each audio file has a corresponding .txt file with the same name.\n",
    "- Audio: `audio1.wav` ‚Üí Transcript: `audio1.txt`\n",
    "- Each .txt file contains the FULL transcript for that audio file\n",
    "- Transcripts will be automatically split into chunks matching audio segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transcript_from_file(transcript_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Load transcript from a .txt file.\n",
    "    \n",
    "    Args:\n",
    "        transcript_path: Path to transcript .txt file\n",
    "        \n",
    "    Returns:\n",
    "        Transcript text as string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(transcript_path, 'r', encoding='utf-8') as f:\n",
    "            transcript = f.read().strip()\n",
    "        return transcript\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error reading {transcript_path}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def load_transcripts_from_directory(\n",
    "    audio_dir: str,\n",
    "    transcript_dir: str\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Load all transcripts from directory.\n",
    "    Matches audio files (*.wav) with transcript files (*.txt).\n",
    "    \n",
    "    Args:\n",
    "        audio_dir: Directory containing audio files\n",
    "        transcript_dir: Directory containing transcript .txt files\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping audio filename to full transcript text\n",
    "        Format: {\"audio1.wav\": \"full transcript text\", ...}\n",
    "    \"\"\"\n",
    "    transcript_dict = {}\n",
    "    \n",
    "    # Get all audio files\n",
    "    audio_files = sorted(Path(audio_dir).glob(\"*.wav\"))\n",
    "    \n",
    "    print(f\"üìÇ Loading transcripts from: {transcript_dir}\")\n",
    "    print(f\"   Found {len(audio_files)} audio files in {audio_dir}\")\n",
    "    \n",
    "    found_count = 0\n",
    "    missing_count = 0\n",
    "    \n",
    "    for audio_path in audio_files:\n",
    "        audio_filename = audio_path.name\n",
    "        # Get corresponding transcript file (same name, .txt extension)\n",
    "        transcript_filename = audio_path.stem + \".txt\"\n",
    "        transcript_path = Path(transcript_dir) / transcript_filename\n",
    "        \n",
    "        if transcript_path.exists():\n",
    "            transcript = load_transcript_from_file(str(transcript_path))\n",
    "            if transcript:\n",
    "                transcript_dict[audio_filename] = transcript\n",
    "                found_count += 1\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  Empty transcript: {transcript_filename}\")\n",
    "                missing_count += 1\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Transcript not found: {transcript_filename}\")\n",
    "            missing_count += 1\n",
    "    \n",
    "    print(f\"\\n‚úÖ Loaded {found_count} transcripts\")\n",
    "    if missing_count > 0:\n",
    "        print(f\"‚ö†Ô∏è  Missing/empty: {missing_count} transcripts\")\n",
    "    \n",
    "    return transcript_dict\n",
    "\n",
    "\n",
    "print(\"‚úÖ Transcript loading functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transcripts from directory\n",
    "transcript_data = load_transcripts_from_directory(\n",
    "    audio_dir=AUDIO_DIR,\n",
    "    transcript_dir=TRANSCRIPT_DIR\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Loaded transcripts for {len(transcript_data)} audio files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample transcripts\n",
    "print(\"\\nüìÑ Sample transcripts:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "sample_count = min(3, len(transcript_data))\n",
    "for i, (filename, transcript) in enumerate(list(transcript_data.items())[:sample_count]):\n",
    "    print(f\"\\n{i+1}. File: {filename}\")\n",
    "    print(f\"   Transcript length: {len(transcript)} characters\")\n",
    "    print(f\"   Preview: {transcript[:150]}...\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "if len(transcript_data) > sample_count:\n",
    "    print(f\"\\n... and {len(transcript_data) - sample_count} more files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataset (NO FILES SAVED!)\n",
    "# This creates offset-based metadata only\n",
    "\n",
    "all_data = process_dataset(\n",
    "    audio_dir=AUDIO_DIR,\n",
    "    transcript_data=transcript_data,\n",
    "    chunk_duration=CHUNK_DURATION,\n",
    "    use_first_n_percent=0.5 if USE_FIRST_50_PERCENT else 1.0\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"  Total samples: {len(all_data)}\")\n",
    "print(f\"  Total duration: {sum(item['duration'] for item in all_data) / 3600:.2f} hours\")\n",
    "print(f\"\\nüìù Sample data point (offset-based):\")\n",
    "if all_data:\n",
    "    print(json.dumps(all_data[0], indent=2, ensure_ascii=False))\n",
    "    print(f\"\\n  Sample transcript length: {len(all_data[0]['text'])} characters\")\n",
    "    print(f\"  Sample transcript preview: {all_data[0]['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìÑ Manifest Generation\n",
    "\n",
    "Create NeMo-compatible JSON manifest files with **offset** and **duration** fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_manifest(\n",
    "    data: List[Dict],\n",
    "    manifest_path: str,\n",
    "    validate: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create NeMo manifest file with offset support.\n",
    "    \n",
    "    Args:\n",
    "        data: List of dicts with audio_filepath, offset, duration, text\n",
    "        manifest_path: Output manifest file path\n",
    "        validate: Whether to validate data before writing\n",
    "    \"\"\"\n",
    "    valid_count = 0\n",
    "    invalid_count = 0\n",
    "    \n",
    "    with open(manifest_path, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            # Validation\n",
    "            if validate:\n",
    "                # Check audio file exists\n",
    "                if not os.path.exists(item['audio_filepath']):\n",
    "                    print(f\"‚ö†Ô∏è  Audio file not found: {item['audio_filepath']}\")\n",
    "                    invalid_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Check text is not empty\n",
    "                if not item['text'] or not item['text'].strip():\n",
    "                    print(f\"‚ö†Ô∏è  Empty text for: {item['audio_filepath']}\")\n",
    "                    invalid_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Check duration is positive\n",
    "                if item['duration'] <= 0:\n",
    "                    print(f\"‚ö†Ô∏è  Invalid duration for: {item['audio_filepath']}\")\n",
    "                    invalid_count += 1\n",
    "                    continue\n",
    "            \n",
    "            # Write to manifest (one JSON per line)\n",
    "            # Include offset for on-the-fly loading\n",
    "            json_line = json.dumps(item, ensure_ascii=False)\n",
    "            f.write(json_line + '\\n')\n",
    "            valid_count += 1\n",
    "    \n",
    "    print(f\"\\n‚úÖ Manifest created: {manifest_path}\")\n",
    "    print(f\"  Valid entries: {valid_count}\")\n",
    "    print(f\"  Format: Offset-based (no chunk files needed)\")\n",
    "    if invalid_count > 0:\n",
    "        print(f\"  ‚ö†Ô∏è  Skipped invalid entries: {invalid_count}\")\n",
    "\n",
    "\n",
    "def train_val_split(\n",
    "    data: List[Dict],\n",
    "    val_ratio: float = 0.1,\n",
    "    shuffle: bool = True,\n",
    "    seed: int = 42\n",
    ") -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Split data into train and validation sets.\n",
    "    \n",
    "    Args:\n",
    "        data: List of data items\n",
    "        val_ratio: Fraction of data for validation\n",
    "        shuffle: Whether to shuffle before splitting\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        (train_data, val_data) tuple\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        np.random.seed(seed)\n",
    "        indices = np.random.permutation(len(data))\n",
    "        data = [data[i] for i in indices]\n",
    "    \n",
    "    split_idx = int(len(data) * (1 - val_ratio))\n",
    "    train_data = data[:split_idx]\n",
    "    val_data = data[split_idx:]\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "\n",
    "print(\"‚úÖ Manifest functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory management utilities\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU and CPU memory cache\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def print_memory_stats():\n",
    "    \"\"\"Print current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "        print(f\"üíæ GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "\n",
    "print(\"‚úÖ Memory management utilities defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/val\n",
    "train_data, val_data = train_val_split(\n",
    "    all_data,\n",
    "    val_ratio=VAL_SPLIT,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"üìä Data Split:\")\n",
    "print(f\"  Training samples: {len(train_data)}\")\n",
    "print(f\"  Validation samples: {len(val_data)}\")\n",
    "print(f\"  Train duration: {sum(item['duration'] for item in train_data) / 3600:.2f} hours\")\n",
    "print(f\"  Val duration: {sum(item['duration'] for item in val_data) / 3600:.2f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create manifest files\n",
    "train_manifest_path = os.path.join(MANIFEST_DIR, \"train_manifest.json\")\n",
    "val_manifest_path = os.path.join(MANIFEST_DIR, \"val_manifest.json\")\n",
    "\n",
    "create_manifest(train_data, train_manifest_path, validate=True)\n",
    "create_manifest(val_data, val_manifest_path, validate=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Manifests ready:\")\n",
    "print(f\"  Train: {train_manifest_path}\")\n",
    "print(f\"  Val: {val_manifest_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Verify manifest samples\n",
    "print(\"üîç Verifying manifest samples...\\n\")\n",
    "\n",
    "def verify_manifest_samples(manifest_path, num_samples=3):\n",
    "    \"\"\"Verify that manifest entries can be loaded correctly\"\"\"\n",
    "    print(f\"Checking: {os.path.basename(manifest_path)}\")\n",
    "    \n",
    "    with open(manifest_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    valid = 0\n",
    "    invalid = 0\n",
    "    \n",
    "    for i, line in enumerate(lines[:num_samples]):\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            \n",
    "            # Check required fields\n",
    "            required = ['audio_filepath', 'offset', 'duration', 'text']\n",
    "            missing = [field for field in required if field not in data]\n",
    "            \n",
    "            if missing:\n",
    "                print(f\"  ‚ùå Entry {i+1}: Missing fields {missing}\")\n",
    "                invalid += 1\n",
    "                continue\n",
    "            \n",
    "            # Check audio file exists\n",
    "            if not os.path.exists(data['audio_filepath']):\n",
    "                print(f\"  ‚ùå Entry {i+1}: Audio file not found: {data['audio_filepath']}\")\n",
    "                invalid += 1\n",
    "                continue\n",
    "            \n",
    "            # Check text is not empty\n",
    "            if not data['text'].strip():\n",
    "                print(f\"  ‚ö†Ô∏è  Entry {i+1}: Empty text\")\n",
    "                invalid += 1\n",
    "                continue\n",
    "            \n",
    "            print(f\"  ‚úÖ Entry {i+1}: OK\")\n",
    "            print(f\"     Audio: {os.path.basename(data['audio_filepath'])}\")\n",
    "            print(f\"     Offset: {data['offset']:.1f}s, Duration: {data['duration']:.1f}s\")\n",
    "            print(f\"     Text: {data['text'][:50]}...\")\n",
    "            valid += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Entry {i+1}: Error - {str(e)}\")\n",
    "            invalid += 1\n",
    "    \n",
    "    print(f\"\\n  Summary: {valid} valid, {invalid} invalid\\n\")\n",
    "\n",
    "# Verify both manifests\n",
    "verify_manifest_samples(train_manifest_path, num_samples=3)\n",
    "verify_manifest_samples(val_manifest_path, num_samples=2)\n",
    "\n",
    "print(\"‚úÖ Manifest verification complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß† Model Fine-tuning\n",
    "\n",
    "Load pretrained model and configure training for Kaggle P100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "print(\"üì• Loading pretrained model: hishab/titu_stt_bn_conformer_large\")\n",
    "print(\"   This may take a few minutes...\\n\")\n",
    "\n",
    "asr_model = nemo_asr.models.ASRModel.from_pretrained(\n",
    "    \"hishab/titu_stt_bn_conformer_large\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully\")\n",
    "print(f\"   Model type: {type(asr_model).__name__}\")\n",
    "print(f\"   Sample rate: {asr_model._cfg.sample_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Memory Troubleshooting Tips\n",
    "\n",
    "If you encounter Out-of-Memory (OOM) errors:\n",
    "\n",
    "**During Training:**\n",
    "- Increase `GRADIENT_ACCUMULATION` to 8 or 16\n",
    "- Keep `BATCH_SIZE = 1`\n",
    "- Set `USE_GRADIENT_CHECKPOINTING = True`\n",
    "- Reduce `MAX_EPOCHS` for faster testing\n",
    "\n",
    "**During Inference:**\n",
    "- Reduce `INFERENCE_BATCH_SIZE` to 2 or 1\n",
    "- Reduce `CHUNK_DURATION` to 15 or 20 seconds\n",
    "- Process test files one at a time (set batch_size=1 in transcribe function)\n",
    "\n",
    "**General:**\n",
    "- Restart kernel to clear all memory\n",
    "- Close other notebooks/processes\n",
    "- Monitor memory with `print_memory_stats()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training data\n",
    "print(\"‚öôÔ∏è  Configuring training data...\")\n",
    "\n",
    "# Update training data config\n",
    "with open_dict(asr_model.cfg):\n",
    "    asr_model.cfg.train_ds.manifest_filepath = train_manifest_path\n",
    "    asr_model.cfg.train_ds.batch_size = BATCH_SIZE\n",
    "    asr_model.cfg.train_ds.shuffle = True\n",
    "    asr_model.cfg.train_ds.num_workers = 2\n",
    "    asr_model.cfg.train_ds.pin_memory = False  # Disabled for memory efficiency\n",
    "    asr_model.cfg.train_ds.sample_rate = SAMPLE_RATE\n",
    "    \n",
    "    # IMPORTANT: Enable offset-based loading\n",
    "    # NeMo's AudioToCharDataset supports offset and duration fields\n",
    "    asr_model.cfg.train_ds.use_start_end_token = False\n",
    "    \n",
    "    # Update validation data config\n",
    "    asr_model.cfg.validation_ds.manifest_filepath = val_manifest_path\n",
    "    asr_model.cfg.validation_ds.batch_size = BATCH_SIZE\n",
    "    asr_model.cfg.validation_ds.shuffle = False\n",
    "    asr_model.cfg.validation_ds.num_workers = 2\n",
    "    asr_model.cfg.validation_ds.sample_rate = SAMPLE_RATE\n",
    "    \n",
    "    # Optimizer config\n",
    "    asr_model.cfg.optim.name = 'adam'\n",
    "    asr_model.cfg.optim.lr = LEARNING_RATE\n",
    "    asr_model.cfg.optim.betas = [0.9, 0.999]\n",
    "    asr_model.cfg.optim.weight_decay = 1e-6\n",
    "    \n",
    "    # Learning rate schedule (warmup + hold + decay)\n",
    "    asr_model.cfg.optim.sched.name = 'CosineAnnealing'\n",
    "    asr_model.cfg.optim.sched.warmup_steps = 500\n",
    "    asr_model.cfg.optim.sched.min_lr = 1e-7\n",
    "    \n",
    "    # Memory optimization: Enable gradient checkpointing if available\n",
    "    if USE_GRADIENT_CHECKPOINTING and hasattr(asr_model.cfg, 'encoder'):\n",
    "        if hasattr(asr_model.cfg.encoder, 'gradient_checkpointing'):\n",
    "            asr_model.cfg.encoder.gradient_checkpointing = True\n",
    "            print(\"‚úÖ Gradient checkpointing enabled\")\n",
    "\n",
    "# Setup training and validation data loaders\n",
    "asr_model.setup_training_data(asr_model.cfg.train_ds)\n",
    "asr_model.setup_validation_data(asr_model.cfg.validation_ds)\n",
    "\n",
    "# Clear memory before training\n",
    "clear_memory()\n",
    "print_memory_stats()\n",
    "\n",
    "print(\"‚úÖ Data configuration complete\")\n",
    "print(\"‚ö° Data loaders will read audio chunks on-the-fly using offsets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure PyTorch Lightning Trainer\n",
    "print(\"‚öôÔ∏è  Configuring trainer...\")\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# Custom callback for memory management\n",
    "class MemoryCleanupCallback(Callback):\n",
    "    \"\"\"Clear GPU cache periodically during training\"\"\"\n",
    "    def __init__(self, cleanup_every_n_steps=10):\n",
    "        self.cleanup_every_n_steps = cleanup_every_n_steps\n",
    "    \n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        if batch_idx % self.cleanup_every_n_steps == 0:\n",
    "            clear_memory()\n",
    "\n",
    "# Checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=CHECKPOINT_DIR,\n",
    "    filename='nemo-asr-{epoch:02d}-{val_wer:.4f}',\n",
    "    monitor='val_wer',\n",
    "    mode='min',\n",
    "    save_top_k=2,  # Reduced to save disk space\n",
    "    save_last=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Early stopping (optional)\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_wer',\n",
    "    patience=3,\n",
    "    mode='min',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Memory cleanup callback\n",
    "memory_callback = MemoryCleanupCallback(cleanup_every_n_steps=EMPTY_CACHE_EVERY_N_STEPS)\n",
    "\n",
    "# Logger\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir='/kaggle/working',\n",
    "    name='nemo_asr_logs'\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    devices=1,\n",
    "    accelerator='gpu',\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    precision='16-mixed',  # Mixed precision for P100\n",
    "    callbacks=[checkpoint_callback, early_stop_callback, memory_callback],\n",
    "    logger=logger,\n",
    "    gradient_clip_val=1.0,\n",
    "    accumulate_grad_batches=GRADIENT_ACCUMULATION,  # Memory-efficient training\n",
    "    log_every_n_steps=10,\n",
    "    val_check_interval=1.0,  # Validate every epoch\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True,\n",
    "    # Memory optimizations\n",
    "    enable_checkpointing=True,\n",
    "    deterministic=False  # Faster training\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer configured\")\n",
    "print(f\"   Devices: {trainer.num_devices} GPU\")\n",
    "print(f\"   Precision: {trainer.precision}\")\n",
    "print(f\"   Max epochs: {MAX_EPOCHS}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Gradient accumulation: {GRADIENT_ACCUMULATION}\")\n",
    "print(f\"   Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   Memory cleanup: Every {EMPTY_CACHE_EVERY_N_STEPS} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"\\nüöÄ Starting fine-tuning...\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Clear memory before training\n",
    "clear_memory()\n",
    "print_memory_stats()\n",
    "\n",
    "trainer.fit(asr_model)\n",
    "\n",
    "# Clear memory after training\n",
    "clear_memory()\n",
    "print_memory_stats()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Resume Training (Optional)\n",
    "\n",
    "If training was interrupted, you can resume from the last checkpoint:\n",
    "\n",
    "```python\n",
    "# Uncomment and run this instead of trainer.fit(asr_model)\n",
    "# checkpoint_path = \"/kaggle/working/checkpoints/last.ckpt\"\n",
    "# trainer.fit(asr_model, ckpt_path=checkpoint_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üíæ Model Export\n",
    "\n",
    "Save the fine-tuned model in NeMo format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to .nemo format\n",
    "print(\"üíæ Saving model...\")\n",
    "\n",
    "# Clear memory before saving\n",
    "clear_memory()\n",
    "\n",
    "asr_model.save_to(FINAL_MODEL_PATH)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {FINAL_MODEL_PATH}\")\n",
    "print(f\"   File size: {os.path.getsize(FINAL_MODEL_PATH) / (1024**3):.2f} GB\")\n",
    "\n",
    "# Delete trainer to free memory\n",
    "del trainer\n",
    "clear_memory()\n",
    "print_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üé§ Test Data Inference & Submission\n",
    "\n",
    "Process test audio files and generate submission CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model for inference\n",
    "print(\"üì• Loading saved model for inference...\")\n",
    "\n",
    "# Clear memory first\n",
    "clear_memory()\n",
    "print_memory_stats()\n",
    "\n",
    "loaded_model = nemo_asr.models.ASRModel.restore_from(FINAL_MODEL_PATH)\n",
    "loaded_model.eval()\n",
    "loaded_model = loaded_model.cuda()  # Move to GPU\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully\")\n",
    "print_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick validation test (optional)\n",
    "# Test model on a validation sample before processing entire test set\n",
    "if val_data and len(val_data) > 0:\n",
    "    print(\"üß™ Quick model validation test...\\n\")\n",
    "    \n",
    "    sample = val_data[0]\n",
    "    \n",
    "    try:\n",
    "        # Extract segment for testing\n",
    "        audio, sr = sf.read(\n",
    "            sample['audio_filepath'],\n",
    "            start=int(sample['offset'] * SAMPLE_RATE),\n",
    "            stop=int((sample['offset'] + sample['duration']) * SAMPLE_RATE)\n",
    "        )\n",
    "        \n",
    "        # Save to temp file\n",
    "        import tempfile\n",
    "        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:\n",
    "            sf.write(tmp.name, audio, SAMPLE_RATE)\n",
    "            prediction = loaded_model.transcribe([tmp.name])[0]\n",
    "            os.unlink(tmp.name)\n",
    "        \n",
    "        print(f\"Ground Truth: {sample['text'][:100]}...\")\n",
    "        print(f\"Prediction:   {prediction[:100]}...\")\n",
    "        print(\"\\n‚úÖ Model working correctly!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Model test failed: {str(e)}\")\n",
    "        print(\"   Check if model loaded correctly and paths are valid\")\n",
    "    \n",
    "    clear_memory()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No validation data available for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all test audio files\n",
    "print(f\"üîç Scanning test directory: {TEST_AUDIO_DIR}\")\n",
    "\n",
    "test_audio_files = sorted(Path(TEST_AUDIO_DIR).glob(\"*.wav\"))\n",
    "print(f\"Found {len(test_audio_files)} test audio files\")\n",
    "\n",
    "if len(test_audio_files) == 0:\n",
    "    print(\"‚ö†Ô∏è  WARNING: No test audio files found! Check TEST_AUDIO_DIR path.\")\n",
    "else:\n",
    "    # Check file sizes and durations\n",
    "    total_duration = 0\n",
    "    print(\"\\nüìä Test dataset info:\")\n",
    "    sample_files = test_audio_files[:3]  # Show first 3\n",
    "    for f in sample_files:\n",
    "        duration = get_audio_duration(str(f))\n",
    "        size_mb = f.stat().st_size / (1024 ** 2)\n",
    "        total_duration += duration\n",
    "        print(f\"  {f.name}: {duration/60:.2f} min, {size_mb:.2f} MB\")\n",
    "    \n",
    "    if len(test_audio_files) > 3:\n",
    "        print(f\"  ... and {len(test_audio_files) - 3} more files\")\n",
    "    \n",
    "    # Estimate total duration\n",
    "    for f in test_audio_files[3:]:\n",
    "        total_duration += get_audio_duration(str(f))\n",
    "    \n",
    "    print(f\"\\n  Total test duration: {total_duration/3600:.2f} hours\")\n",
    "    print(f\"  Average file duration: {total_duration/len(test_audio_files)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate inference time\n",
    "if len(test_audio_files) > 0:\n",
    "    # Rough estimate: ~1.5x real-time for P100 (30s audio takes ~45s to process)\n",
    "    avg_duration_per_file = total_duration / len(test_audio_files)\n",
    "    processing_time_per_file = avg_duration_per_file * 1.5  # Conservative estimate\n",
    "    total_estimated_time = (processing_time_per_file * len(test_audio_files)) / 60  # in minutes\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è  Estimated inference time:\")\n",
    "    print(f\"   ~{total_estimated_time:.1f} minutes for {len(test_audio_files)} files\")\n",
    "    print(f\"   (~{total_estimated_time/60:.1f} hours)\")\n",
    "    print(f\"\\nüí° Tip: Adjust INFERENCE_BATCH_SIZE to speed up or reduce memory usage\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_long_audio_chunked(\n",
    "    model,\n",
    "    audio_path: str,\n",
    "    chunk_duration: float = 30.0,\n",
    "    sample_rate: int = 16000,\n",
    "    batch_size: int = 4\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Transcribe long audio file by chunking and merging transcripts.\n",
    "    Memory-efficient: processes in batches without saving chunks.\n",
    "    \n",
    "    Args:\n",
    "        model: NeMo ASR model\n",
    "        audio_path: Path to audio file\n",
    "        chunk_duration: Duration of each chunk in seconds\n",
    "        sample_rate: Target sample rate\n",
    "        batch_size: Number of chunks to process at once\n",
    "        \n",
    "    Returns:\n",
    "        Complete transcript (merged from all chunks)\n",
    "    \"\"\"\n",
    "    import tempfile\n",
    "    \n",
    "    # Get audio info\n",
    "    audio_info = sf.info(audio_path)\n",
    "    total_duration = audio_info.duration\n",
    "    \n",
    "    # Calculate chunks\n",
    "    num_chunks = int(np.ceil(total_duration / chunk_duration))\n",
    "    \n",
    "    all_transcripts = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for batch_start in range(0, num_chunks, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, num_chunks)\n",
    "        temp_files = []\n",
    "        \n",
    "        try:\n",
    "            # Extract batch of chunks to temp files\n",
    "            for i in range(batch_start, batch_end):\n",
    "                offset = i * chunk_duration\n",
    "                duration = min(chunk_duration, total_duration - offset)\n",
    "                \n",
    "                # Read segment\n",
    "                audio_segment, sr = sf.read(\n",
    "                    audio_path,\n",
    "                    start=int(offset * sample_rate),\n",
    "                    stop=int((offset + duration) * sample_rate)\n",
    "                )\n",
    "                \n",
    "                # Resample if needed\n",
    "                if sr != sample_rate:\n",
    "                    audio_segment = librosa.resample(\n",
    "                        audio_segment,\n",
    "                        orig_sr=sr,\n",
    "                        target_sr=sample_rate\n",
    "                    )\n",
    "                \n",
    "                # Save to temp file\n",
    "                tmp = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)\n",
    "                sf.write(tmp.name, audio_segment, sample_rate)\n",
    "                temp_files.append(tmp.name)\n",
    "                tmp.close()\n",
    "                \n",
    "                # Clear memory\n",
    "                del audio_segment\n",
    "            \n",
    "            # Transcribe batch\n",
    "            batch_transcripts = model.transcribe(temp_files)\n",
    "            all_transcripts.extend(batch_transcripts)\n",
    "            \n",
    "        finally:\n",
    "            # Clean up temp files\n",
    "            for tmp_file in temp_files:\n",
    "                try:\n",
    "                    os.unlink(tmp_file)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Clear memory\n",
    "            clear_memory()\n",
    "    \n",
    "    # Merge transcripts with spaces\n",
    "    full_transcript = \" \".join(all_transcripts)\n",
    "    return full_transcript\n",
    "\n",
    "\n",
    "print(\"‚úÖ Chunked transcription function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all test files\n",
    "print(\"\\nüé§ Starting test inference...\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for audio_path in tqdm(test_audio_files, desc=\"Transcribing test files\"):\n",
    "    filename = audio_path.name\n",
    "    \n",
    "    try:\n",
    "        # Get audio duration\n",
    "        duration = get_audio_duration(str(audio_path))\n",
    "        \n",
    "        # Choose transcription method based on duration\n",
    "        if duration > CHUNK_DURATION:\n",
    "            # Long audio: use chunked transcription\n",
    "            transcript = transcribe_long_audio_chunked(\n",
    "                loaded_model,\n",
    "                str(audio_path),\n",
    "                chunk_duration=CHUNK_DURATION,\n",
    "                sample_rate=SAMPLE_RATE,\n",
    "                batch_size=INFERENCE_BATCH_SIZE\n",
    "            )\n",
    "        else:\n",
    "            # Short audio: transcribe directly\n",
    "            transcript = loaded_model.transcribe([str(audio_path)])[0]\n",
    "        \n",
    "        # Store result\n",
    "        results.append({\n",
    "            \"filename\": filename,\n",
    "            \"transcript\": transcript\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è  Error processing {filename}: {str(e)}\")\n",
    "        # Add empty transcript for failed files\n",
    "        results.append({\n",
    "            \"filename\": filename,\n",
    "            \"transcript\": \"\"\n",
    "        })\n",
    "    \n",
    "    # Clear memory periodically\n",
    "    if len(results) % 5 == 0:\n",
    "        clear_memory()\n",
    "\n",
    "print(f\"\\n‚úÖ Inference complete! Processed {len(results)} files\")\n",
    "print_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Generate Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission DataFrame\n",
    "print(\"üìù Creating submission file...\")\n",
    "\n",
    "submission_df = pd.DataFrame(results)\n",
    "submission_df = submission_df[[\"filename\", \"transcript\"]]\n",
    "\n",
    "# Remove .wav extension from filenames\n",
    "submission_df[\"filename\"] = submission_df[\"filename\"].str.replace(r\"\\.wav$\", \"\", regex=True)\n",
    "\n",
    "# Fill any empty transcriptions\n",
    "submission_df[\"transcript\"] = submission_df[\"transcript\"].fillna(\"\")\n",
    "\n",
    "# Save submission\n",
    "submission_csv_path = SUBMISSION_PATH + \"submission.csv\"\n",
    "submission_df.to_csv(submission_csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"‚úÖ Submission saved to: {submission_csv_path}\")\n",
    "print(f\"   Total rows: {len(submission_df)}\")\n",
    "\n",
    "# Display preview\n",
    "print(f\"\\nüìÑ Submission preview:\")\n",
    "print(submission_df.head(10))\n",
    "\n",
    "# Statistics\n",
    "print(f\"\\nüìä Submission statistics:\")\n",
    "print(f\"   Files with transcripts: {(submission_df['transcript'] != '').sum()}\")\n",
    "print(f\"   Empty transcripts: {(submission_df['transcript'] == '').sum()}\")\n",
    "print(f\"   Average transcript length: {submission_df['transcript'].str.len().mean():.1f} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Display some example transcriptions\n",
    "print(\"\\nüéØ Sample transcriptions:\\n\")\n",
    "\n",
    "for i in range(min(5, len(submission_df))):\n",
    "    row = submission_df.iloc[i]\n",
    "    print(f\"File: {row['filename']}\")\n",
    "    print(f\"Transcript: {row['transcript'][:200]}{'...' if len(row['transcript']) > 200 else ''}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Clear memory at end\n",
    "clear_memory()\n",
    "print(\"\\n‚úÖ All done!\")\n",
    "print_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Summary\n",
    "\n",
    "### Completed Steps:\n",
    "\n",
    "1. ‚úÖ **Setup & Installation**: Installed NeMo toolkit and dependencies\n",
    "2. ‚úÖ **Transcript Loading**: Loaded transcripts from individual .txt files (one per audio file)\n",
    "3. ‚úÖ **Audio Preprocessing**: Created offset-based chunk metadata (NO files saved)\n",
    "4. ‚úÖ **Transcript Chunking**: Automatically split full transcripts into segments matching audio chunks\n",
    "5. ‚úÖ **Manifest Generation**: Created NeMo JSON manifests with offset/duration fields\n",
    "6. ‚úÖ **Model Fine-tuning**: Fine-tuned pretrained Bangla Conformer on P100 with memory optimizations\n",
    "7. ‚úÖ **Model Export**: Saved model to `.nemo` format\n",
    "8. ‚úÖ **Test Inference**: Processed all test audio files with chunked transcription\n",
    "9. ‚úÖ **Submission Generation**: Created CSV file ready for submission\n",
    "\n",
    "### üéØ Data Processing Features:\n",
    "\n",
    "- üìÇ **Individual Transcript Files**: Each audio file (e.g., `audio1.wav`) has a corresponding transcript file (`audio1.txt`)\n",
    "- ‚úÇÔ∏è **Smart Transcript Splitting**: Full transcripts automatically split into chunks based on sentence boundaries\n",
    "- üìä **Configurable Dataset Size**: Use `USE_FIRST_50_PERCENT` to train on subset of data (memory/time optimization)\n",
    "- ‚öñÔ∏è **Intelligent Chunking**: Distributes sentences evenly across audio segments\n",
    "\n",
    "### üíæ Memory Optimization Features:\n",
    "\n",
    "- ‚ö° **Gradient Accumulation**: Simulates larger batch sizes without memory overhead\n",
    "- ‚ö° **Mixed Precision (FP16)**: Reduces memory usage by ~50%\n",
    "- ‚ö° **Gradient Checkpointing**: Trades computation for memory\n",
    "- ‚ö° **Periodic Cache Clearing**: Prevents memory fragmentation\n",
    "- ‚ö° **On-the-fly Audio Loading**: No intermediate chunk files saved\n",
    "- ‚ö° **Batch Inference**: Processes multiple test chunks efficiently\n",
    "- ‚ö° **Pin Memory Disabled**: Reduces CPU memory overhead\n",
    "- üìâ **50% Data Option**: Train on first half of dataset for faster iterations\n",
    "\n",
    "### üéØ Key Benefits:\n",
    "\n",
    "- ‚ö° **No intermediate files**: Saves disk space and processing time\n",
    "- ‚ö° **On-the-fly loading**: NeMo reads segments directly from original files\n",
    "- ‚ö° **Memory efficient**: Optimized for P100 16GB VRAM\n",
    "- ‚ö° **Handles long audio**: Automatic chunking for files > 30 seconds\n",
    "- ‚ö° **Flexible transcripts**: Works with full transcripts per audio file\n",
    "- ‚ö° **Production ready**: Complete inference pipeline with error handling\n",
    "\n",
    "### üìÅ Output Files:\n",
    "\n",
    "- **Train Manifest**: `/kaggle/working/processed_data/manifests/train_manifest.json`\n",
    "- **Val Manifest**: `/kaggle/working/processed_data/manifests/val_manifest.json`\n",
    "- **Checkpoints**: `/kaggle/working/checkpoints/`\n",
    "- **Final Model**: `/kaggle/working/nemo_bangla_asr_finetuned.nemo`\n",
    "- **Submission**: `/kaggle/working/submission.csv`\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "1. ‚úÖ Download `submission.csv` for competition submission\n",
    "2. üí° Monitor TensorBoard logs for training insights\n",
    "3. üîß Fine-tune hyperparameters if needed (learning rate, batch size, etc.)\n",
    "4. üìä Evaluate WER on validation set\n",
    "5. üéØ Further optimize for better accuracy\n",
    "6. üîÑ If needed, train on full dataset by setting `USE_FIRST_50_PERCENT = False`\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Notes:\n",
    "\n",
    "- **Transcript Format**: Each audio file needs a corresponding .txt file with the same base name\n",
    "- **Dataset Size**: Currently using first 50% of training data (controlled by `USE_FIRST_50_PERCENT`)\n",
    "- **Transcript Splitting**: Transcripts are split by sentence boundaries for better alignment\n",
    "- **Inference**: Adjust `INFERENCE_BATCH_SIZE` if you encounter OOM during test inference\n",
    "- **Long Audio**: Files are automatically chunked into 30-second segments\n",
    "- **Error Handling**: Empty transcripts are handled gracefully (filled with empty strings)\n",
    "- **Memory**: All audio chunks are processed in-memory without saving files\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
